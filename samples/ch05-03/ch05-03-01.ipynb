{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Word2Vec 利用事例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前提\n",
    "\n",
    "次のライブラリがpipコマンドで導入済みであること\n",
    "\n",
    "```\n",
    "pip install wikipedia\n",
    "pip install janome\n",
    "pip install gensim\n",
    "conda install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.1 初期処理と変数宣言\n",
    "\n",
    "# Macの問題回避\n",
    "import os\n",
    "import platform\n",
    "if platform.system() == 'Darwin':\n",
    "    os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "    \n",
    "# waring抑止\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Word2Vecの隠れ層ノード数\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# LSTMで保持する時系列データ数\n",
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト5.3.2 テキスト取得\n",
    "# wikipedia ライブラリを利用\n",
    "\n",
    "import wikipedia\n",
    "wikipedia.set_lang(\"ja\")\n",
    "\n",
    "# 学習データ(歴史)のタイトル\n",
    "list1 = ['大和時代', '奈良時代', '平安時代', '鎌倉時代', '室町時代', '安土桃山時代', '江戸時代',\n",
    "        '藤原道長','平清盛','源頼朝', '北条早雲','伊達政宗','徳川家康', '武田信玄', '上杉謙信', \n",
    "         '今川義元', '毛利元就', '足利尊氏', '足利義満', '北条泰時']\n",
    "\n",
    "# 学習データ(地理)のタイトル\n",
    "list2 = ['東北地方', '関東地方', '中部地方', '近畿地方', '中国地方', '四国地方', '九州地方',\n",
    "        '北海道', '秋田県', '福島県', '宮城県', '新潟県', '長野県', '山梨県', '静岡県', '愛知県', \n",
    "         '栃木県', '群馬県', '千葉県', '神奈川県']\n",
    "\n",
    "# テストデータのタイトル\n",
    "list3 = ['織田信長', '豊臣秀吉', '青森県', '北海道']\n",
    "\n",
    "# それぞれのタイトルに対してWikipediaのsummaryを取得し、配列に保存\n",
    "list1_w = [wikipedia.summary(item) for item in list1]\n",
    "list2_w = [wikipedia.summary(item) for item in list2]\n",
    "list3_w = [wikipedia.summary(item) for item in list3]\n",
    "\n",
    "# すべての取得結果を 1つのlistに集約\n",
    "list_all_w = list1_w + list2_w + list3_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.3 テキストに対して単語毎にブランクを入れる\n",
    "# 形態素解析としてJanomeを利用\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "def wakati(text):\n",
    "    w = t.tokenize(text, wakati=True)\n",
    "    return ' '.join(w)\n",
    "\n",
    "list1_x = [wakati(w) for w in list1_w]\n",
    "list2_x = [wakati(w) for w in list2_w]\n",
    "list3_x = [wakati(w) for w in list3_w]\n",
    "list_all_x = list1_x + list2_x + list3_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.4 学習データ作成\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 学習・検証で使う全テキストを引数にして辞書を作成する\n",
    "tokenizer.fit_on_texts(list_all_x)\n",
    "\n",
    "# 単語一覧を取得\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 総単語数を取得\n",
    "num_words = len(word_index)\n",
    "print('総単語数: ', num_words)\n",
    "\n",
    "# 変換前の検証用テキスト確認\n",
    "print('変換前テキスト: ', list3_x[0])\n",
    "\n",
    "# テキストの数値化\n",
    "sequence_test = tokenizer.texts_to_sequences(list3_x)\n",
    "\n",
    "# 変換結果確認\n",
    "print('変換後: ', sequence_test[0])\n",
    "\n",
    "# 単語のパディング(短いときは0で埋める、長いときは途中で切る)\n",
    "sequence_test = pad_sequences(sequence_test, maxlen=MAX_LEN)\n",
    "\n",
    "# 変換結果確認\n",
    "print('パディング後: ', sequence_test[0])\n",
    "\n",
    "# 学習データに対しても同じ変換を行う\n",
    "sequence_train = tokenizer.texts_to_sequences(list1_x + list2_x)\n",
    "sequence_train = pad_sequences(sequence_train, maxlen=MAX_LEN)\n",
    "\n",
    "# 正解データ作成\n",
    "# y = 0: 歴史  \n",
    "# y = 1: 地理  \n",
    "    \n",
    "Y_train = np.array([0] * len( list1_x) + [1] * len( list2_x))\n",
    "Y_test = np.array([0] * 2 + [1] * 2)\n",
    "print('正解データ(学習用): ', Y_train)\n",
    "print('正解データ(検証用): ', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.5 学習済みWord2Vecロード\n",
    "\n",
    "# ja.binファイルの準備方法は5.2節を参照のこと\n",
    "\n",
    "import gensim \n",
    "word_vectors = gensim.models.Word2Vec.load('ja.bin')\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.6 Embedding Matrix作成\n",
    "import numpy as np\n",
    "\n",
    "# num_words: 辞書作成時に検出された単語数 (リスト 5.3.4)\n",
    "# EMBEDDING_DIM: Word2Vec 隠れ層のノード数 (リスト 5.3.1)\n",
    "# メモリー領域節約のため、float32を利用します\n",
    "embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "# Embedding MatrixにWord2Vecの重みベクトル値をコピー\n",
    "for word, i in word_index.items():\n",
    "    if word in word_vectors.wv.vocab:\n",
    "        embedding_matrix[i] = word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.7 LSTMモデル作成\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words+1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False))\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.8 学習\n",
    "\n",
    "# モデル学習の実施\n",
    "model.fit(sequence_train, Y_train,validation_data=(\n",
    "    sequence_test, Y_test), batch_size=128, verbose=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.9 予測・評価\n",
    "\n",
    "# 検証データの内容\n",
    "for text in list3_w:\n",
    "    print(text)\n",
    "\n",
    "# 評価\n",
    "model.predict(sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
